{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Workplace Reviews and Text Classification through an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other day I was talking to a contractor, who enthusiastically told me about the stock market application of sentiment analysis. Inspired, I decided to do a little analysis of my own, but rather than analyzing stocks, I will instead analyze reviews of the Texas Workforce Commission.\n",
    "\n",
    "To get reviews, I'll scraping data off of the [Consumer Affairs](https://www.consumeraffairs.com/) website.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape data from the website, I'll need the reqests library to send http requests, and the BeautifulSoup library to parse the content of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each page in the 7 pages of reviews, I'll request the url, then parse the html content.\n",
    "\n",
    "In the code below, I passed the url of the first webpage to the get function, which returned a *response* object. The content of the webpage is an attribute of the object, which I pass to the BeautifulSoup function. The BeautifulSoup function returns a parser object, which I can use to efficiently parse the html code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.consumeraffairs.com/employment/tx_work.html?page='\n",
    "response = requests.get(url+\"1\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the portion of the html code with the reviews, I opened the webpage in Google Chrome, right-clicked the web page, then clicked \"Inspect\" to see the CSS class of html tag I wanted.\n",
    "\n",
    "The CSS class is \"ca-txt-bd-2\", which I'll pass into the select method of the parser. The select method will then return a list of all the html code with the CSS class.\n",
    "\n",
    "Below is the text from the first item in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page 1\\n        \\n        Reviews 1 - 30\\n    '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_box = parser.select(\".ca-txt-bd-2\")\n",
    "review_box[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the text of the second item in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Original review: March 14, 2019I put in my unemployment as I was supposed to, did job searches, filled out their dumb** forms. Was denied for a month, then I put in the second round. It took I got my check 1 and a half weeks later. Then the week after that I get a notice saying I had to pay back everything plus 150 dollars due to overpayment because it was sent to Oklahoma, I had it sent to Oklahoma due to a family issue I had to tend to. I live in Texas, have a Texas license, was outta town a week and half, still did my online job searches. Now I have to pay back money plus extra just because, it's **. I don't even have a job to pay it back, they even said on the notice ignoring could result in charges and a jail sentence.\\n    (adsbygoogle = window.adsbygoogle || []).push({\\n        params: {\\n            google_ad_channel: 9398056344,\\n            google_ad_client: 'ca-pub-0200629403145096',\\n            google_ad_type: 'text',\\n            google_override_format: true,\\n            google_color_link: '#166BA1',\\n            google_color_text: '#000000',\\n            google_color_bg: '#F0F0F0',\\n            google_color_url: '#333333'\\n        }\\n    });\\n    \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_box[1].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll need to remove \"(adsbygoogle\" portion of the text, since this is not part of the review.\n",
    "\n",
    "Let's make a function that does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_ad(text):\n",
    "    index = text.find(\"(adsbygoogle\")\n",
    "    if index != -1:\n",
    "        return text[0:(index-1)]\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing said function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Original review: March 14, 2019I put in my unemployment as I was supposed to, did job searches, filled out their dumb** forms. Was denied for a month, then I put in the second round. It took I got my check 1 and a half weeks later. Then the week after that I get a notice saying I had to pay back everything plus 150 dollars due to overpayment because it was sent to Oklahoma, I had it sent to Oklahoma due to a family issue I had to tend to. I live in Texas, have a Texas license, was outta town a week and half, still did my online job searches. Now I have to pay back money plus extra just because, it's **. I don't even have a job to pay it back, they even said on the notice ignoring could result in charges and a jail sentence.\\n   \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_ad(review_box[1].get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, my strategy is to make a list of reviews. For each page, I'll parse the html content, search for the CSS class with the review, remove the first element, then for each review I'll remove the ad from the text and append the text to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = []\n",
    "for i in range(1,8):\n",
    "    response = requests.get(url+str(i))\n",
    "    content = response.content\n",
    "    parser = BeautifulSoup(content,'html.parser')\n",
    "    review_box = parser.select(\".ca-txt-bd-2\")\n",
    "    for j in range(1,len(review_box)):\n",
    "        text = remove_ad(review_box[j].get_text())\n",
    "        reviews.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test that my algorithm worked correctly, I'll check that there are 183 reviews in my list of reviews. If the length of my list is 183, then this length will match the number displayed on the first webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! On to the API reqests!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Requests to text-processing.com API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found [this](https://text-processing.com/demo/sentiment/) website, which demos Sentiment Analysis using NLTK 2.0.4. The demo takes text as input, then outputs 1 of 3 classes of text, which is either positive, negative or neutral. \n",
    "\n",
    "Upon further reading, I found that the webiste has a fairly simple API, with 1,000 free requests per IP. For each request sent, the API will return a JSON dictionary with the keys probability and label. The value for the probability key is just another dictionary with the keys pos, neg, and neutral, whose values are the probabilities for each text class. The value for the label key is the predicted text class.\n",
    "\n",
    "To demonstrate I'll send a request for the first review and get the JSON dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'neutral',\n",
       " 'probability': {'neg': 0.8254697386652051,\n",
       "  'neutral': 0.905510239593793,\n",
       "  'pos': 0.17453026133479488}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = { \"text\" : reviews[0] }\n",
    "response = requests.post('http://text-processing.com/api/sentiment/', data=data)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy, right? Now, I just gotta do this 183 more times.\n",
    "\n",
    "My plan of attack is to loop through the list of reviews, and store all the data in a dictionary of dictionaries.\n",
    "\n",
    "Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_dict = {}\n",
    "for i in range(len(reviews)):\n",
    "    review_dict[i] = {}\n",
    "    review_dict[i]['text'] = reviews[i]\n",
    "    data = { 'text' : reviews[i] }\n",
    "    response = requests.post('http://text-processing.com/api/sentiment/', data=data)\n",
    "    json_dict = response.json()\n",
    "    review_dict[i]['label'] = json_dict['label']\n",
    "    review_dict[i]['neg_prob'] = json_dict['probability']['neg']\n",
    "    review_dict[i]['neutral_prob'] = json_dict['probability']['neutral']\n",
    "    review_dict[i]['pos_prob'] = json_dict['probability']['pos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the Best Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 183 reviews. Of the few that I looked at, all seemed pretty negative. Let's see which review was the most positive.\n",
    "\n",
    "I'll loop through the keys of the dictionary, and find the key with highest positive probability value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_prob = review_dict[0]['pos_prob']\n",
    "max = 0\n",
    "for i in range(1,len(reviews)):\n",
    "    prob = review_dict[i]['pos_prob']\n",
    "    if prob > max_prob:\n",
    "        max_prob = prob\n",
    "        max = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drum roll please...\n",
    "\n",
    "And the best review is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Original review: May 1, 2015Says they're going to send you a check in two, and then sends a letter asking for more information. This cycle has happened twice. Children are starving, and people are suffering because TX can't get its act together. People are entitled to unemployment compensation after losing a job. That's what we get for living in Texas, I guess. Well, not anymore. TX want to play games with people's fundamental rights. We will be moving to a state that respects and treats their workforce with dignity. Thanks, TX... for teaching us how not to act.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_dict[max]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews of the Texas Workforce Commission are pretty negative.\n",
    "\n",
    "I had some fun scraping reviews from a website and sending http requests to an API.\n",
    "\n",
    "Thanks for reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
